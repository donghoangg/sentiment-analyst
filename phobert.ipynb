{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install transformers[torch]\n",
        "!pip install pandas scikit-learn matplotlib seaborn\n",
        "!pip install pyvi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kLFTD0u6-ZJ",
        "outputId": "c35be99d-9a5f-4e8f-ff0b-fd8a077fe233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (4.67.1)\n",
            "Requirement already satisfied: torch<2.7,>=2.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (1.7.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (1.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.1->transformers[torch]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.7,>=2.1->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<2.7,>=2.1->transformers[torch]) (3.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting pyvi\n",
            "  Downloading pyvi-0.1.1-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from pyvi) (1.6.1)\n",
            "Collecting sklearn-crfsuite (from pyvi)\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (3.6.0)\n",
            "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite->pyvi)\n",
            "  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (4.67.1)\n",
            "Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.11 pyvi-0.1.1 sklearn-crfsuite-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864
        },
        "id": "j8CKsygF6Vjj",
        "outputId": "8743364a-b04b-4714-fbe8-ea5c90a57248"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Bắt đầu trực quan hóa dữ liệu ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Đã lưu biểu đồ phân bố dữ liệu vào data_distribution.png\n",
            "--- Kết thúc trực quan hóa dữ liệu ---\n",
            "\n",
            "--- Tải và chuẩn bị dữ liệu cho việc huấn luyện ---\n",
            "\n",
            "--- Bắt đầu quá trình huấn luyện PhoBERT ---\n",
            "Đang xử lý văn bản...\n",
            "Số lượng lớp (nhãn): 2\n",
            "Chia dữ liệu train/validation...\n",
            "Khởi tạo mô hình PhoBERT để phân loại...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bắt đầu quá trình Fine-tuning PhoBERT...\n",
            "Epoch 1/3\n",
            "----------\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import torch\n",
        "import seaborn as sns\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
        "from pyvi import ViTokenizer # Thư viện quan trọng để tách từ tiếng Việt\n",
        "\n",
        "# =====================================================================================\n",
        "# PHẦN 1: TẢI VÀ TRỰC QUAN HÓA DỮ LIỆU (Giữ nguyên logic của bạn)\n",
        "# =====================================================================================\n",
        "\n",
        "PATH_DATA = \"/content/train.crash\"\n",
        "\n",
        "class DataSource:\n",
        "    def _load_raw_data(self, filename, is_train=True):\n",
        "        a = []\n",
        "        b = []\n",
        "        regex = 'train_' if is_train else 'test_'\n",
        "        try:\n",
        "            with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
        "                for line in file:\n",
        "                    if regex in line:\n",
        "                        if a:\n",
        "                            b.append(a)\n",
        "                        a = [line]\n",
        "                    elif line.strip():\n",
        "                        a.append(line)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Lỗi: Không tìm thấy file {filename}\")\n",
        "            return []\n",
        "        if a:\n",
        "            b.append(a)\n",
        "        return b[1:] if b and regex in b[0][0] else b\n",
        "\n",
        "    def _create_row(self, sample_lines, is_train=True):\n",
        "        d = {}\n",
        "        if not sample_lines:\n",
        "            return None\n",
        "        d['id'] = sample_lines[0].strip()\n",
        "        if is_train:\n",
        "            if len(sample_lines) < 2:\n",
        "                 print(f\"Cảnh báo: Sample không đủ dòng: {sample_lines}\")\n",
        "                 return None\n",
        "            review_lines = sample_lines[1:-1]\n",
        "            label_line = sample_lines[-1].strip()\n",
        "            if not label_line.isdigit():\n",
        "                print(f\"Cảnh báo: Nhãn không hợp lệ cho sample {d['id']}: '{label_line}'.\")\n",
        "                review_lines = sample_lines[1:]\n",
        "                d['label'] = -1\n",
        "            else:\n",
        "                d['label'] = int(label_line)\n",
        "        else:\n",
        "            review_lines = sample_lines[1:]\n",
        "        d['review'] = \"\".join(clause.strip() for clause in review_lines)\n",
        "        if not d['review']:\n",
        "            print(f\"Cảnh báo: Review rỗng cho ID {d['id']}\")\n",
        "        return d\n",
        "\n",
        "    def load_data(self, filename, is_train=True):\n",
        "        raw_samples = self._load_raw_data(filename, is_train)\n",
        "        processed_rows = [self._create_row(s, is_train) for s in raw_samples if self._create_row(s, is_train)]\n",
        "        if not processed_rows:\n",
        "            print(f\"Không có dữ liệu nào được tải từ {filename}. Vui lòng kiểm tra định dạng file.\")\n",
        "        return processed_rows\n",
        "\n",
        "def visualize_data(file_path):\n",
        "    ds = DataSource()\n",
        "    data_list = ds.load_data(file_path, is_train=True)\n",
        "    if not data_list:\n",
        "        print(\"Không có dữ liệu để trực quan hóa.\")\n",
        "        return\n",
        "    train_data = pd.DataFrame(data_list)\n",
        "    if 'label' not in train_data.columns or train_data.empty:\n",
        "        print(\"Dữ liệu không có cột 'label' hoặc rỗng sau khi tải.\")\n",
        "        return\n",
        "    valid_labels = train_data[train_data['label'] != -1]['label']\n",
        "    if valid_labels.empty:\n",
        "        print(\"Không có nhãn hợp lệ để trực quan hóa.\")\n",
        "        return\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.hist(valid_labels.to_numpy(), bins=len(valid_labels.unique()), rwidth=0.8, align='left')\n",
        "    plt.xlabel(\"Class (Label)\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.title(\"Phân bố dữ liệu huấn luyện\")\n",
        "    plt.xticks(sorted(valid_labels.unique()))\n",
        "    plt.savefig('data_distribution.png')\n",
        "    plt.show()\n",
        "    print(\"Đã lưu biểu đồ phân bố dữ liệu vào data_distribution.png\")\n",
        "\n",
        "# =====================================================================================\n",
        "# PHẦN 2: TIỀN XỬ LÝ VĂN BẢN (Cập nhật cho PhoBERT)\n",
        "# =====================================================================================\n",
        "\n",
        "class Util:\n",
        "    dict_replace = {\n",
        "        \"ship\": \"vận chuyển\",\"shop\": \"cửa hàng\",\"m\": \"mình\",\"mik\":\"mình\",\"ko\":\"không\",\"k\":\"không\",\"kh\":\"không\",\"khong\":\"không\",\"kg\":\"không\",\"khg\":\"không\",\"tl\":\"trả lời\",\n",
        "\"rep\":\"trả lời\",\"r\":\"rồi\",\"fb\":\"facebook\",\"face\":\"faceook\",\"thanks\":\"cảm ơn\",\"thank\":\"cảm ơn\",\"tks\":\"cảm ơn\",\"tk\":\"cảm ơn\",\"ok\":\"tốt\",\"oki\":\"tốt\",\"okie\":\"tốt\",\"sp\":\"sản phẩm\",\n",
        "\"dc\":\"được\",\"vs\":\"với\",\"đt\":\"điện thoại\",\"thjk\":\"thích\",\"thik\":\"thích\",\"qá\":\"quá\",\"trể\":\"trễ\",\"bgjo\":\"bao giờ\",\"h\":\"giờ\",\"qa\":\"quá\",\"dep\":\"đẹp\",\"xau\":\"xấu\",\"ib\":\"nhắn tin\",\n",
        "\"cute\":\"dễ thương\",\"sz\":\"size\",\"good\":\"tốt\",\"god\":\"tốt\",\"bt\":\"bình thường\"\n",
        "    }\n",
        "\n",
        "    def remove_redundant(self, text):\n",
        "        if not isinstance(text, str): return \"\"\n",
        "        return re.sub(r'([A-Z])\\1+', lambda m: m.group(1).upper(), text, flags=re.IGNORECASE)\n",
        "\n",
        "    def normalize(self, text):\n",
        "        if not isinstance(text, str): return \"\"\n",
        "        text = text.lower()\n",
        "        words = text.split()\n",
        "        words = [self.dict_replace.get(w, w) for w in words]\n",
        "        return \" \".join(words)\n",
        "\n",
        "    def process_text(self, text):\n",
        "        text = self.remove_redundant(text)\n",
        "        text = self.normalize(text)\n",
        "        # BƯỚC QUAN TRỌNG: Tách từ tiếng Việt cho PhoBERT\n",
        "        text = ViTokenizer.tokenize(text)\n",
        "        return text\n",
        "\n",
        "# =====================================================================================\n",
        "# PHẦN 3: HUẤN LUYỆN MÔ HÌNH PHO BERT (Thay thế hoàn toàn)\n",
        "# =====================================================================================\n",
        "\n",
        "# Tạo một lớp Dataset cho PyTorch\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.texts[item])\n",
        "        label = self.labels[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Lớp chính để điều khiển quá trình huấn luyện\n",
        "class PhobertTrainer:\n",
        "    def __init__(self, model_name=\"vinai/phobert-base-v2\", max_len=256, batch_size=16, epochs=3):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.max_len = max_len\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.util = Util()\n",
        "\n",
        "    def create_data_loader(self, df, tokenizer, max_len, batch_size):\n",
        "        ds = SentimentDataset(\n",
        "            texts=df.review.to_numpy(),\n",
        "            labels=df.label.to_numpy(),\n",
        "            tokenizer=tokenizer,\n",
        "            max_len=max_len\n",
        "        )\n",
        "        return DataLoader(ds, batch_size=batch_size, num_workers=2)\n",
        "\n",
        "    def train_epoch(self, model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "        model = model.train()\n",
        "        losses = []\n",
        "        correct_predictions = 0\n",
        "        for d in data_loader:\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            labels = d[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            _, preds = torch.max(logits, dim=1)\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "    def eval_model(self, model, data_loader, loss_fn, device, n_examples):\n",
        "        model = model.eval()\n",
        "        losses = []\n",
        "        correct_predictions = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for d in data_loader:\n",
        "                input_ids = d[\"input_ids\"].to(device)\n",
        "                attention_mask = d[\"attention_mask\"].to(device)\n",
        "                labels = d[\"labels\"].to(device)\n",
        "\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels\n",
        "                )\n",
        "                loss = outputs.loss\n",
        "                logits = outputs.logits\n",
        "\n",
        "                _, preds = torch.max(logits, dim=1)\n",
        "                correct_predictions += torch.sum(preds == labels)\n",
        "                losses.append(loss.item())\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        accuracy = correct_predictions.double() / n_examples\n",
        "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "        return accuracy, np.mean(losses), f1, all_labels, all_preds\n",
        "\n",
        "    def run_training(self, df):\n",
        "        print(\"Đang xử lý văn bản...\")\n",
        "        df['review'] = df['review'].apply(self.util.process_text)\n",
        "\n",
        "        # Lấy số lượng nhãn duy nhất\n",
        "        num_labels = df.label.nunique()\n",
        "        print(f\"Số lượng lớp (nhãn): {num_labels}\")\n",
        "\n",
        "        print(\"Chia dữ liệu train/validation...\")\n",
        "        df_train, df_val = train_test_split(df, test_size=0.2, random_state=42, stratify=df.label)\n",
        "\n",
        "        train_data_loader = self.create_data_loader(df_train, self.tokenizer, self.max_len, self.batch_size)\n",
        "        val_data_loader = self.create_data_loader(df_val, self.tokenizer, self.max_len, self.batch_size)\n",
        "\n",
        "        print(\"Khởi tạo mô hình PhoBERT để phân loại...\")\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=num_labels)\n",
        "        model = model.to(self.device)\n",
        "\n",
        "        optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "        total_steps = len(train_data_loader) * self.epochs\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=0,\n",
        "            num_training_steps=total_steps\n",
        "        )\n",
        "        loss_fn = torch.nn.CrossEntropyLoss().to(self.device)\n",
        "\n",
        "        print(\"Bắt đầu quá trình Fine-tuning PhoBERT...\")\n",
        "        for epoch in range(self.epochs):\n",
        "            print(f'Epoch {epoch + 1}/{self.epochs}')\n",
        "            print('-' * 10)\n",
        "\n",
        "            train_acc, train_loss = self.train_epoch(\n",
        "                model, train_data_loader, loss_fn, optimizer, self.device, scheduler, len(df_train)\n",
        "            )\n",
        "            print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "            val_acc, val_loss, val_f1, _, _ = self.eval_model(\n",
        "                model, val_data_loader, loss_fn, self.device, len(df_val)\n",
        "            )\n",
        "            print(f'Val loss {val_loss} accuracy {val_acc} F1-score {val_f1}')\n",
        "            print()\n",
        "\n",
        "        print(\"Huấn luyện hoàn tất.\")\n",
        "\n",
        "        # Đánh giá cuối cùng và vẽ ma trận nhầm lẫn\n",
        "        print(\"Đánh giá trên tập validation và vẽ ma trận nhầm lẫn...\")\n",
        "        _, _, _, y_test, y_pred = self.eval_model(model, val_data_loader, loss_fn, self.device, len(df_val))\n",
        "\n",
        "        class_labels = sorted(df['label'].unique())\n",
        "        cm = confusion_matrix(y_test, y_pred, labels=class_labels)\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                    xticklabels=class_labels, yticklabels=class_labels)\n",
        "        plt.title(\"Confusion Matrix on Validation Set\")\n",
        "        plt.ylabel(\"True Label\")\n",
        "        plt.xlabel(\"Predicted Label\")\n",
        "        plt.show()\n",
        "\n",
        "        return model, self.tokenizer\n",
        "\n",
        "# =====================================================================================\n",
        "# PHẦN 4: PHÂN LOẠI CÂU MỚI (Cập nhật)\n",
        "# =====================================================================================\n",
        "\n",
        "def classify_sentence(sentence, model, tokenizer, device, max_len=256):\n",
        "    util = Util()\n",
        "    processed_text = util.process_text(sentence)\n",
        "\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        processed_text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_len,\n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        _, prediction = torch.max(logits, dim=1)\n",
        "\n",
        "    # Giả định nhãn 0 là tích cực, 1 là tiêu cực.\n",
        "    # Bạn có thể tạo một map rõ ràng hơn nếu cần: label_map = {0: 'Tích cực', 1: 'Tiêu cực'}\n",
        "    if prediction.item() == 1:\n",
        "        return \"Bình luận tiêu cực!\"\n",
        "    elif prediction.item() == 0:\n",
        "        return \"Bình luận tích cực!\"\n",
        "    else:\n",
        "        return f\"Bình luận được phân loại là: {prediction.item()}\"\n",
        "\n",
        "# =====================================================================================\n",
        "# PHẦN 5: HÀM MAIN ĐIỀU KHIỂN CHÍNH\n",
        "# =====================================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 1. Khởi tạo DataSource và trực quan hóa\n",
        "    ds = DataSource()\n",
        "    print(\"--- Bắt đầu trực quan hóa dữ liệu ---\")\n",
        "    visualize_data(PATH_DATA)\n",
        "    print(\"--- Kết thúc trực quan hóa dữ liệu ---\\n\")\n",
        "\n",
        "    # 2. Tải và làm sạch dữ liệu\n",
        "    print(\"--- Tải và chuẩn bị dữ liệu cho việc huấn luyện ---\")\n",
        "    raw_data_list = ds.load_data(PATH_DATA, is_train=True)\n",
        "    if not raw_data_list:\n",
        "        print(\"Không thể tải dữ liệu, chương trình kết thúc.\")\n",
        "    else:\n",
        "        data_df = pd.DataFrame(raw_data_list)\n",
        "        # Loại bỏ các hàng có nhãn không hợp lệ hoặc review rỗng\n",
        "        data_df = data_df[data_df['label'] != -1]\n",
        "        data_df = data_df.dropna(subset=['review'])\n",
        "        data_df = data_df[data_df['review'].str.strip() != '']\n",
        "\n",
        "        if data_df.empty:\n",
        "            print(\"Không có dữ liệu hợp lệ sau khi làm sạch.\")\n",
        "        else:\n",
        "            # 3. Huấn luyện mô hình\n",
        "            print(\"\\n--- Bắt đầu quá trình huấn luyện PhoBERT ---\")\n",
        "            trainer = PhobertTrainer(epochs=3) # Có thể tăng số epochs nếu cần\n",
        "            trained_model, trained_tokenizer = trainer.run_training(data_df)\n",
        "\n",
        "            # 4. Lưu model đã huấn luyện\n",
        "            model_save_path = '/content/phobert_sentiment_model.pkl'\n",
        "            trained_model.save_pretrained(model_save_path)\n",
        "            trained_tokenizer.save_pretrained(model_save_path)\n",
        "            print(f\"\\nĐã lưu model và tokenizer vào thư mục: {model_save_path}\")\n",
        "\n",
        "            # 5. Vòng lặp kiểm tra phân loại câu mới\n",
        "            print(\"\\n--- Bắt đầu kiểm tra phân loại câu ---\")\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "            while True:\n",
        "                text_input = input(\"Nhập câu để kiểm tra cảm xúc (hoặc nhập 'exit' để thoát): \").strip()\n",
        "                if text_input.lower() == \"exit\":\n",
        "                    break\n",
        "                if not text_input:\n",
        "                    print(\"Vui lòng nhập một câu.\")\n",
        "                    continue\n",
        "\n",
        "                classification_result = classify_sentence(\n",
        "                    text_input,\n",
        "                    trained_model,\n",
        "                    trained_tokenizer,\n",
        "                    device\n",
        "                )\n",
        "                print(f\"Kết quả: {classification_result}\")\n",
        "\n",
        "    print(\"\\n--- Chương trình kết thúc ---\")"
      ]
    }
  ]
}